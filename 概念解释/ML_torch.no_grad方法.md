## torch.no_grad()

PyTorch 中的一个上下文管理器，用于临时关闭梯度计算。其主要作用是在不需要梯度计算时（如模型推理或验证阶段）禁用自动求导，从而节省内存和提升运算速度。

具体来说，当代码块被 torch.no_grad() 包围时，所有计算得到的张量都会自动设置 requires_grad=False，即不会被记录到计算图中，不会计算梯度。这样避免了为反向传播保存中间状态，降低了内存使用。典型用法是在推断模型时避免不必要的梯度计算，提高效率。

它既可以用作上下文管理器（with torch.no_grad():），也可以用作装饰器（@torch.no_grad()）来装饰函数。