## [CLS]（Classification Token）
放在输入序列开头，用来聚合整个输入序列的信息。模型通过对[CLS]对应输出向量进行处理，实现句子或文本的分类任务，如情感分析、文本分类等。它代表了整个序列的综合语义表达，是下游任务常用的整体序列表示 。

## [SEP] token 
是自然语言处理（NLP）和机器学习模型中使用的特殊分隔符标记，用来标记不同文本段落或句子之间的边界。它主要用途是让模型区分输入文本中不同的部分，比如在问答任务中区分问题和上下文，或者在句子对任务中分隔两个句子，从而帮助模型更好地理解和处理上下文关系 。

在基于 Transformer 的模型（如 BERT、Sentence Transformer）中，[SEP] token 设计用来分割多个输入序列，有助于模型进行句子级别的关联计算和推理。常见用法是把输入格式化为 [CLS] 句子A [SEP] 句子B [SEP]，这样模型能够清晰地知道哪里是第一个句子结尾，哪里是第二个句子的开始。省略或错用该 token 会导致模型表现下降或理解错误 。

## [PAD]（Padding Token）
用于对齐批处理时序列长度，使输入在批量训练中长度一致。其对模型无语义贡献，仅用于占位。模型会忽略这些位置的输入，保持计算效率和结果一致 。

## [MASK]（Mask Token）
在预训练阶段用来掩盖输入中的部分词汇，模型任务是预测这些被掩盖的词，促使模型学习上下文语义关系。常用于掩码语言模型（如BERT）训练，是自监督学习的关键
