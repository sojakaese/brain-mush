# 2. 数据准备
## 2.1 当前主流RAG文档加载器
主流文档加载器工具：

- PyMuPDF4LLM	PDF→Markdown转换，OCR+表格识别	
- TextLoader	基础文本文件加载
- DirectoryLoader	批量目录文件处理	混合格式文档库	支持多格式扩展
- Unstructured	多格式文档解析	PDF、Word、HTML等	统一接口，智能解析
- FireCrawlLoader	网页内容抓取	在线文档、新闻	实时内容获取
- LlamaParse	深度PDF结构解析	法律合同、学术论文	解析精度高，商业API
- Docling	模块化企业级解析	企业合同、报告	IBM生态兼容
- Marker	PDF→Markdown，GPU加速	科研文献、书籍	专注PDF转换
- MinerU	多模态集成解析	学术文献、财务报表	集成LayoutLMv3+YOLOv8

## 2.1.2 Unstructured
这里特别介绍Unstructured，专门设计用于RAG和AI微调场景的非结构化数据预处理
- 支持多种文档格式：PDF、Word、Excel、HTML、Markdown等
- 能识别文档元素类型：标题、段落、表格、列表等结构元素
- 保留文档元数据信息

**代码示例要点**
通过partition类来解析文档的元素。

## 2.2 文本分块 Text Chunking
目的：
- 满足核心组件上下文限制的需求：嵌入模型和大语言模型
- 满足检索生成的性能需求

### 2.2.2 文本分块的意义
1. 减少嵌入过程的信息损失
1. 避免文本块过长，关键信息噪声太多，引起大模型提取答案的性能下降
2. 避免单块文本过长导致的主题过多和语义稀释

### 2.2.3 langchain 提供的文本分块策略
- CharacterTextSplitter 基础策略，根据**默认分隔符**和Regex来分割然后合并，用chunk_size和chunk_overlap两个参数来调整分块大小。
- RecursiveCharacterTextSplitter 递归策略，定义有优先级的多个分隔符，用递归方式处理不同优先级满足条件的文本块，递归的最内层为用尽分隔符的时候，然后分割合并。适合syntaxs结构完整的文本，比如代码或者元数据清晰的文本。
- SemanticChunker 语义策略，
- TextSplitter基于文档结构的实现，比如 Markdown，XML，JSon，HTML
- 用Splitter结合递归策略来实现更复杂的文本分块。
  
### 2.2.4 其他框架提供的文本分块策略
- Unstructured: 先以结构分区 Partitioning, 再分块 Chunking  
- LlamaIndex: 对数据进行抽象为节点Node，然后再操作，比如 结构节点，语义节点
- ChunkViz：类似WordCloud的文本块可视化工具

## 练习
trivia